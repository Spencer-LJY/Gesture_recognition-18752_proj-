{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !unzip /content/data/HandGesture.zip\n",
        "!unzip -q datasets.zip"
      ],
      "metadata": {
        "id": "YYzsXoPy5hi7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/data/TestImages.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRHIlCzkIgTq",
        "outputId": "453e97f2-55d9-4d6a-b23e-e7f7fc27a193"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/data/TestImages.zip\n",
            "  inflating: Test Images/img_2.png   \n",
            "  inflating: Test Images/img_0.png   \n",
            "  inflating: Test Images/img_1.png   \n",
            "  inflating: Test Images/img_3.png   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVkW1eaIopB_",
        "outputId": "938f8f75-6275-4f06-aa5c-ab7308e1c8ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 96, 116, 64)       1664      \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 96, 116, 64)      256       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 48, 58, 64)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 48, 58, 64)        0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 44, 54, 64)        102464    \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 44, 54, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 22, 27, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 22, 27, 64)        0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 18, 23, 64)        102464    \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 18, 23, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 9, 11, 64)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 9, 11, 64)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 6336)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               811136    \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,019,786\n",
            "Trainable params: 1,019,402\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, BatchNormalization\n",
        "from keras.layers import Activation, Dropout\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# First conv layer\n",
        "model.add(Conv2D(64, kernel_size=(5, 5), activation='relu', input_shape=(100,120, 1))) \n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# Second conv layer\n",
        "model.add(Conv2D(64, kernel_size=(5, 5), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# Thrid conv layer\n",
        "model.add(Conv2D(64, kernel_size=(5, 5), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# Flatten\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu')) \n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Softmax \n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "\n",
        "optimiser = Adam() \n",
        "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "\n",
        "# Load dataset\n",
        "DATASET_PATH = 'datasets/HandGesture/images/'\n",
        "\n",
        "dataset_path = os.path.join(DATASET_PATH, '*')\n",
        "dataset_path = glob.glob(dataset_path)"
      ],
      "metadata": {
        "id": "WGjGMGTEo7Hw"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_images = []\n",
        "\n",
        "gestures_list = ['call_me', 'fingers_crossed', 'okay', 'paper', 'peace', 'rock', 'rock_on', 'scissor', 'thumbs', 'up']\n",
        "\n",
        "for p in range(len(dataset_path)):\n",
        "\n",
        "    print(gestures_list[p])\n",
        "\n",
        "    # dataset_path1 = \"/content/HandGesture/images/\" + str(gestures_list[p])\n",
        "    dataset_path1 = DATASET_PATH + str(gestures_list[p])\n",
        "    gesture_path = os.path.join(dataset_path1, '*')\n",
        "\n",
        "    g_path = glob.glob(gesture_path)\n",
        "    k = 0\n",
        "    for i in range(0, len(g_path)):\n",
        "        if k < 1600:\n",
        "            image = cv2.imread(g_path[i])\n",
        "            gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "            gray_image = cv2.resize(gray_image,(100, 120))\n",
        "            input_images.append(gray_image)\n",
        "        k+=1\n",
        "\n",
        "print(len(input_images))\n",
        "\n",
        "labels = []\n",
        "for i in range(527):\n",
        "    labels.append([1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
        "\n",
        "for i in range(504):\n",
        "    labels.append([0, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
        "\n",
        "for i in range(540):\n",
        "    labels.append([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n",
        "    \n",
        "for i in range(539):\n",
        "    labels.append([0, 0, 0, 1, 0, 0, 0, 0, 0, 0])\n",
        "    \n",
        "for i in range(526):\n",
        "    labels.append([0, 0, 0, 0, 1, 0, 0, 0, 0, 0])\n",
        "\n",
        "for i in range(508):\n",
        "    labels.append([0, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
        "\n",
        "for i in range(531):\n",
        "    labels.append([0, 0, 0, 0, 0, 0, 1, 0, 0, 0])\n",
        "\n",
        "for i in range(527):\n",
        "    labels.append([0, 0, 0, 0, 0, 0, 0, 1, 0, 0])\n",
        "    \n",
        "for i in range(537):\n",
        "    labels.append([0, 0, 0, 0, 0, 0, 0, 0, 1, 0])\n",
        "    \n",
        "for i in range(504):\n",
        "    labels.append([0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n",
        "\n",
        "\n",
        "print(len(labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1ZsjCYnpoih",
        "outputId": "fd694b59-3af5-437d-cf25-3857d574ed6f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "call_me\n",
            "fingers_crossed\n",
            "okay\n",
            "paper\n",
            "peace\n",
            "rock\n",
            "rock_on\n",
            "scissor\n",
            "thumbs\n",
            "up\n",
            "5243\n",
            "5243\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = np.asarray(input_images)\n",
        "y = np.asarray(labels)\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvsJrFRDzXzu",
        "outputId": "1c96510a-ae79-4fb8-ab48-9232a092a2da"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5243, 120, 100)\n",
            "(5243, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)\n",
        "X_train = X_train.reshape(X_train.shape[0], 100, 120, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 100, 120, 1)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMrpiLr4xQ_N",
        "outputId": "4eb21be9-9a4e-44e2-8eb1-8420bb55acba"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4194, 100, 120, 1)\n",
            "(1049, 100, 120, 1)\n",
            "(4194, 10)\n",
            "(1049, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train, batch_size=64, epochs=30, verbose=1, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EN97zauc1yeM",
        "outputId": "3b6e5035-4781-4882-bd3e-f107628d176b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "66/66 [==============================] - 2s 22ms/step - loss: 0.3910 - categorical_accuracy: 0.8708 - val_loss: 0.5722 - val_categorical_accuracy: 0.7950\n",
            "Epoch 2/30\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.2908 - categorical_accuracy: 0.9032 - val_loss: 0.5906 - val_categorical_accuracy: 0.8275\n",
            "Epoch 3/30\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.1893 - categorical_accuracy: 0.9418 - val_loss: 0.2962 - val_categorical_accuracy: 0.9304\n",
            "Epoch 4/30\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.1599 - categorical_accuracy: 0.9447 - val_loss: 0.2565 - val_categorical_accuracy: 0.9476\n",
            "Epoch 5/30\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.1613 - categorical_accuracy: 0.9497 - val_loss: 0.9544 - val_categorical_accuracy: 0.8360\n",
            "Epoch 6/30\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.1225 - categorical_accuracy: 0.9571 - val_loss: 0.0946 - val_categorical_accuracy: 0.9800\n",
            "Epoch 7/30\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.0958 - categorical_accuracy: 0.9738 - val_loss: 0.1371 - val_categorical_accuracy: 0.9685\n",
            "Epoch 8/30\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.1069 - categorical_accuracy: 0.9683 - val_loss: 0.1337 - val_categorical_accuracy: 0.9733\n",
            "Epoch 9/30\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.0998 - categorical_accuracy: 0.9683 - val_loss: 0.0889 - val_categorical_accuracy: 0.9809\n",
            "Epoch 10/30\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.0822 - categorical_accuracy: 0.9750 - val_loss: 0.1008 - val_categorical_accuracy: 0.9752\n",
            "Epoch 11/30\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.0647 - categorical_accuracy: 0.9783 - val_loss: 0.1330 - val_categorical_accuracy: 0.9762\n",
            "Epoch 12/30\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.0433 - categorical_accuracy: 0.9843 - val_loss: 0.0475 - val_categorical_accuracy: 0.9895\n",
            "Epoch 13/30\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.0756 - categorical_accuracy: 0.9776 - val_loss: 0.1465 - val_categorical_accuracy: 0.9638\n",
            "Epoch 14/30\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.0856 - categorical_accuracy: 0.9728 - val_loss: 0.4864 - val_categorical_accuracy: 0.8961\n",
            "Epoch 15/30\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.0781 - categorical_accuracy: 0.9764 - val_loss: 0.0548 - val_categorical_accuracy: 0.9886\n",
            "Epoch 16/30\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.0575 - categorical_accuracy: 0.9807 - val_loss: 0.0815 - val_categorical_accuracy: 0.9867\n",
            "Epoch 17/30\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.0489 - categorical_accuracy: 0.9859 - val_loss: 0.1931 - val_categorical_accuracy: 0.9676\n",
            "Epoch 18/30\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.0670 - categorical_accuracy: 0.9816 - val_loss: 0.2046 - val_categorical_accuracy: 0.9695\n",
            "Epoch 19/30\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.0546 - categorical_accuracy: 0.9833 - val_loss: 0.1465 - val_categorical_accuracy: 0.9714\n",
            "Epoch 20/30\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.0517 - categorical_accuracy: 0.9824 - val_loss: 0.1117 - val_categorical_accuracy: 0.9800\n",
            "Epoch 21/30\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.0362 - categorical_accuracy: 0.9888 - val_loss: 0.1817 - val_categorical_accuracy: 0.9752\n",
            "Epoch 22/30\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.0811 - categorical_accuracy: 0.9795 - val_loss: 0.1742 - val_categorical_accuracy: 0.9571\n",
            "Epoch 23/30\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.0665 - categorical_accuracy: 0.9816 - val_loss: 3.5738 - val_categorical_accuracy: 0.8017\n",
            "Epoch 24/30\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.0735 - categorical_accuracy: 0.9809 - val_loss: 0.0527 - val_categorical_accuracy: 0.9886\n",
            "Epoch 25/30\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.0391 - categorical_accuracy: 0.9878 - val_loss: 0.0490 - val_categorical_accuracy: 0.9905\n",
            "Epoch 26/30\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.0567 - categorical_accuracy: 0.9864 - val_loss: 0.1030 - val_categorical_accuracy: 0.9733\n",
            "Epoch 27/30\n",
            "66/66 [==============================] - 1s 14ms/step - loss: 0.0779 - categorical_accuracy: 0.9766 - val_loss: 0.0824 - val_categorical_accuracy: 0.9867\n",
            "Epoch 28/30\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.0443 - categorical_accuracy: 0.9864 - val_loss: 0.0754 - val_categorical_accuracy: 0.9905\n",
            "Epoch 29/30\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.0586 - categorical_accuracy: 0.9835 - val_loss: 0.0625 - val_categorical_accuracy: 0.9895\n",
            "Epoch 30/30\n",
            "66/66 [==============================] - 1s 15ms/step - loss: 0.0410 - categorical_accuracy: 0.9895 - val_loss: 0.0885 - val_categorical_accuracy: 0.9876\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa2ca3f1300>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[loss, acc] = model.evaluate(X_test,y_test)\n",
        "print(\"Accuracy: \" + str(acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9dEHXO61yzk",
        "outputId": "e8ce9072-cd1a-447b-fa22-83a3eebc62c8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33/33 [==============================] - 0s 4ms/step - loss: 0.0885 - categorical_accuracy: 0.9876\n",
            "Accuracy: 0.9876072406768799\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save/load the model!\n",
        "model.save('nn_model')\n",
        "model = keras.models.load_model(\"nn_model\")\n",
        "!zip -r nn_model.zip nn_model/ "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McmEvxAPPiVI",
        "outputId": "ed9369bf-f8be-4897-e42a-4d080c80b5e0"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: nn_model/ (stored 0%)\n",
            "  adding: nn_model/assets/ (stored 0%)\n",
            "  adding: nn_model/saved_model.pb (deflated 89%)\n",
            "  adding: nn_model/variables/ (stored 0%)\n",
            "  adding: nn_model/variables/variables.data-00000-of-00001 (deflated 8%)\n",
            "  adding: nn_model/variables/variables.index (deflated 64%)\n",
            "  adding: nn_model/keras_metadata.pb (deflated 93%)\n",
            "  adding: nn_model/.zip (stored 0%)\n",
            "  adding: nn_model/fingerprint.pb (stored 0%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zdNRzeYRLOn",
        "outputId": "8b38e7b6-6326-4b24-b8ce-9281d726f829"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "datasets  datasets.zip\tnn_model  nn_model.zip\tsample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "image = cv2.imread('/content/Test Images/img_0.png')\n",
        "image = cv2.resize(image,(100, 120))\n",
        "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "gray_image = cv2.resize(gray_image, (100, 120))\n",
        "gray_image = gray_image.reshape(1, 100, 120, 1)\n",
        "\n",
        "pred = model.predict(gray_image)\n",
        "y = np.argmax(pred)\n",
        "print(\"Predict class:\",y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "Q0akJ1nQ5Opw",
        "outputId": "4e8bf667-1818-4023-e584-372bf66f6270"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-5ebcf6154ea5>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/Test Images/img_0.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m120\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.7.0) /io/opencv/modules/imgproc/src/resize.cpp:4062: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QNY9NCnXBR32"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}